{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIyNjiDX0wVlVzgaC49ncF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MRifdaM/ML_S5_2025/blob/main/Week1/TG1_2341720028_Muhamad_Rifda_Musyafa'.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BAGIAN 1"
      ],
      "metadata": {
        "id": "8SZXTv2Y3Hb7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmTTX89pqz7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ddf9d35-8f73-4e99-e704-c9f33a1c60e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprep\n",
            "  Downloading pyprep-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting mne>=1.3.0 (from pyprep)\n",
            "  Downloading mne-1.10.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.12/dist-packages (from pyprep) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.12/dist-packages (from pyprep) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.12/dist-packages (from pyprep) (1.16.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne>=1.3.0->pyprep) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.7->mne>=1.3.0->pyprep) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.3.0->pyprep) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.3.0->pyprep) (2.32.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.3.0->pyprep) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->mne>=1.3.0->pyprep) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.3.0->pyprep) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.3.0->pyprep) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.3.0->pyprep) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.3.0->pyprep) (2025.8.3)\n",
            "Downloading pyprep-0.5.0-py3-none-any.whl (34 kB)\n",
            "Downloading mne-1.10.1-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne, pyprep\n",
            "Successfully installed mne-1.10.1 pyprep-0.5.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pyprep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbH-7sCzezW_",
        "outputId": "5ace4c1b-177b-4a27-ad60-e4a54420613e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.35.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neurokit2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8DywO_Zz2bF",
        "outputId": "ac18f6f9-ef73-4353-f905-dba3ff0f0f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neurokit2\n",
            "  Downloading neurokit2-0.2.12-py2.py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from neurokit2) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from neurokit2) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from neurokit2) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (3.10.0)\n",
            "Requirement already satisfied: PyWavelets>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from neurokit2) (1.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5.0->neurokit2) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->neurokit2) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->neurokit2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->neurokit2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->neurokit2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->neurokit2) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->neurokit2) (1.17.0)\n",
            "Downloading neurokit2-0.2.12-py2.py3-none-any.whl (708 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.4/708.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neurokit2\n",
            "Successfully installed neurokit2-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Jelaskan kegunakan masing-masing library tersebut.\n",
        ">PyPREP:\n",
        "PyPREP (Python Preprocessing) adalah sebuah toolkit untuk melakukan preprocessing pada data EEG (elektroensefalografi). Fungsi utamanya adalah untuk membersihkan data EEG dari artefak yang tidak diinginkan, seperti artefak gerakan, kedipan mata, atau gangguan dari perangkat.\n",
        "\n",
        ">Scipy:\n",
        "Scipy adalah pustaka Python yang digunakan untuk komputasi ilmiah dan teknis. Ini menyediakan berbagai modul untuk optimasi, aljabar linear, integrasi, interpolasi, statistik, pemrosesan sinyal, dan banyak lagi.  Scipy sering digunakan bersama dengan NumPy untuk menangani array multidimensional dan operasi matematis yang kompleks.\n",
        "\n",
        ">wandb:\n",
        "wandb (Weights & Biases) adalah sebuah platform untuk melacak, memvisualisasikan, dan menganalisis eksperimen machine learning.  Fungsi utamanya adalah membantu para peneliti dan insinyur untuk memantau performa model, membandingkan berbagai eksperimen, dan menyimpan hasil pelatihan dalam satu tempat yang terorganisir.\n",
        ">Menyediakan tools untuk:\n",
        "\n",
        ">>Mencatat hasil training model (loss, accuracy, dll.).\n",
        "\n",
        ">>Visualisasi metrik secara real-time di dashboard online.\n",
        "\n",
        ">>Membandingkan eksperimen (misalnya model dengan parameter berbeda).\n",
        "\n",
        ">>Menyimpan model, dataset, dan log untuk kolaborasi tim.\n",
        "\n",
        ">pyECG:\n",
        "pyECG adalah sebuah library untuk pemrosesan data EKG (elektrokardiogram). Kegunaan utamanya adalah untuk menganalisis sinyal EKG, seperti mendeteksi gelombang P, kompleks QRS, dan gelombang T, serta menghitung detak jantung. Library ini mempermudah para peneliti dan praktisi medis untuk mengekstraksi informasi penting dari data EKG.\n",
        "Fungsinya biasanya mencakup:\n",
        "\n",
        ">>Preprocessing: filtering, denoising sinyal jantung.\n",
        "\n",
        ">>Deteksi peak (R-peak, QRS complex) → untuk menghitung HRV (Heart Rate Variability) dan detak jantung.\n",
        "\n",
        ">>Ekstraksi fitur ECG untuk analisis kesehatan jantung.\n",
        "\n",
        ">NeuroKit2: NeuroKit2 adalah library Python yang didesain untuk memproses sinyal neurofisiologis, seperti sinyal yang berasal dari tubuh manusia. Ini adalah toolkit yang ramah pengguna dan bersifat open-source yang mempermudah para peneliti, ilmuwan, dan klinisi untuk menganalisis data fisiologis. Fiturnya jauh lebih luas dibanding PyECG:\n",
        "\n",
        ">>Preprocessing sinyal (filtering, cleaning).\n",
        "\n",
        ">>Deteksi R-peaks.\n",
        "\n",
        ">>Analisis HRV (Heart Rate Variability).\n",
        "\n",
        ">>Analisis lain (EDA, EMG, RESP, EEG).\n"
      ],
      "metadata": {
        "id": "4IDNdwCCMrI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BAGIAN 2\n"
      ],
      "metadata": {
        "id": "CdPaA6mo3S2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Berikan contoh tentang tidakan melanggar etika dan hukum tentang penggunaan kecerdasan buatan. Sertakan referensi yang digunakan.\n",
        "\n",
        ">a. Pengacara Australia mengeksploitasi AI secara ilegal di persidangan\n",
        "\n",
        ">Seorang pengacara di Victoria, Australia, dihukum karena menyerahkan daftar yurisprudensi palsu yang dibuat oleh AI tanpa melakukan verifikasi. Meskipun meminta maaf, ia dikenai sanksi profesional—tidak boleh praktek sebagai penanggung utama, mengelola dana kepercayaan, atau menjalankan firma hukum selama dua tahun.\n",
        "\n",
        ">src: https://www.theguardian.com/law/2025/sep/03/lawyer-caught-using-ai-generated-false-citations-in-court-case-penalised-in-australian-first\n",
        "\n",
        ">b. Deepfake audio bersifat rasis & antisemitik—hukum\n",
        "\n",
        ">Dazhon Darien, mantan direktur atletik sekolah menengah di Maryland, dijatuhi hukuman 4 bulan penjara setelah membuat klip deepfake audio yang menggambarkan sang kepala sekolah mengeluarkan ujaran kebencian terhadap orang kulit hitam dan Yahudi. Insiden ini bukan hanya melanggar etika, tetapi juga berdampak hukum serius.\n",
        "\n",
        ">src: https://apnews.com/article/racist-ai-recording-maryland-high-school-487ea673b0449077cb23e7970546cb9f\n",
        "\n",
        "#2. Berikan contoh tentang dampak energi dan lingkungan terhadap pemanfaatan kecerdasan buatan dan bagaimana cara mengatasinya (Anda dapat memberikan pendapat pribadi). Sertakan referensi yang digunakan.\n",
        "\n",
        ">a. Dampak Energi\n",
        "\n",
        ">Penggunaan AI, khususnya pada sistem Gemini milik Google, membutuhkan energi setiap kali pengguna mengirimkan prompt. Hasil pengukuran menyeluruh menunjukkan bahwa satu prompt rata-rata mengonsumsi 0,24 Wh energi. Energi ini terbagi pada beberapa lapisan infrastruktur:\n",
        "\n",
        ">>58% (0,14 Wh) digunakan oleh akselerator AI yang memproses model (seperti TPU).\n",
        "\n",
        ">>25% (0,06 Wh) berasal dari konsumsi CPU dan DRAM host yang wajib berjalan bersama akselerator.\n",
        "\n",
        ">>10% (0,02 Wh) digunakan oleh mesin cadangan yang harus tetap aktif agar layanan selalu siap (low latency dan high availability).\n",
        "\n",
        ">>8% (0,02 Wh) berasal dari energi overhead pusat data, termasuk pendinginan dan sistem distribusi daya.\n",
        "\n",
        ">Untuk memberikan gambaran, energi 0,24 Wh ini setara dengan menyalakan televisi modern selama 9 detik. Artinya, dampak energi dari satu prompt terlihat kecil jika dilihat secara individu. Namun, karena Google melayani miliaran prompt setiap hari, total konsumsi energinya menjadi sangat besar di skala global.\n",
        "\n",
        ">Cara Mengatasi:\n",
        "\n",
        ">Google mengurangi konsumsi energi melalui beberapa strategi efisiensi. Pertama, mereka merancang arsitektur model yang lebih hemat, misalnya Mixture-of-Experts (MoE) yang hanya mengaktifkan bagian model yang relevan dengan prompt tertentu. Kedua, mereka menerapkan algoritma efisien seperti quantization dan speculative decoding untuk mengurangi jumlah komputasi. Ketiga, mereka menggunakan perangkat keras khusus TPU Ironwood yang terbukti 30 kali lebih hemat energi dibanding generasi TPU pertama. Selain itu, pusat data Google dijalankan dengan efisiensi tinggi, memiliki PUE rata-rata 1,09, artinya hanya ada 9% energi tambahan untuk pendinginan dan overhead.\n",
        "\n",
        ">Pendapat saya:\n",
        "\n",
        ">Menurut saya, cara paling efektif untuk mengatasi dampak energi adalah membangun efisiensi sejak tahap desain model dan sistem. Jika pengembang hanya berfokus pada optimalisasi di tahap akhir, hasilnya tidak akan signifikan. Oleh karena itu, saya berpendapat bahwa pengembang sebaiknya sejak awal memilih algoritma yang ringan, memanfaatkan model kecil untuk kebutuhan spesifik, serta mendorong penggunaan perangkat keras yang memang didesain untuk efisiensi. Dengan begitu, jejak energi AI dapat ditekan secara berkelanjutan, meskipun penggunaannya semakin luas.\n",
        "\n",
        ">b. Dampak Emisi Karbon\n",
        "\n",
        ">Energi yang digunakan AI tentu menghasilkan emisi karbon. Satu prompt di Gemini menghasilkan rata-rata 0,03 gram CO₂e. Meskipun angka ini terlihat kecil, jika dikalikan miliaran prompt, total emisi menjadi signifikan. Namun, Google mencatat penurunan emisi per prompt yang drastis: 44 kali lipat dalam kurun waktu 12 bulan.\n",
        "\n",
        ">Cara Mengatasi:\n",
        "\n",
        ">Google menekan emisi dengan dua cara utama. Pertama, menurunkan konsumsi energi per prompt melalui arsitektur model efisien, algoritma, dan perangkat keras hemat energi. Kedua, mengurangi intensitas emisi listrik yang digunakan pusat data melalui pembelian energi bersih dan upaya menuju operasi dengan 24/7 carbon-free energy.\n",
        "\n",
        ">Pendapat saya:\n",
        "\n",
        ">Menurut saya, mengatasi dampak emisi tidak cukup hanya dengan efisiensi teknis. Kuncinya ada pada transisi energi bersih. Jika pusat data masih bergantung pada listrik dari pembangkit berbahan bakar fosil, emisi tetap akan tinggi meskipun modelnya efisien. Karena itu, saya berpendapat penting bagi pengembang AI untuk bermitra dengan penyedia energi terbarukan atau bahkan membangun infrastruktur energi hijau sendiri. Dengan begitu, AI tidak hanya efisien secara komputasi, tetapi juga benar-benar ramah lingkungan dalam jangka panjang.\n",
        "\n",
        ">c. Dampak Konsumsi Air\n",
        "\n",
        ">Selain energi dan emisi, konsumsi air menjadi perhatian besar. Google mencatat bahwa satu prompt mengonsumsi 0,26 mililiter air, setara dengan lima tetes air. Air ini digunakan untuk mendinginkan server di pusat data. Jumlah ini jauh lebih rendah dibanding perkiraan dari penelitian lain yang menyebutkan bisa mencapai 10-50 mL per prompt.\n",
        "\n",
        ">Tetapi, meskipun konsumsi per prompt kecil, totalnya tetap signifikan pada skala miliaran prompt. Apalagi, air adalah sumber daya yang rentan di wilayah dengan stres air tinggi.\n",
        "\n",
        ">Cara Mengatasi:\n",
        "\n",
        ">Google menerapkan Responsible Water Use Framework. Strategi utamanya adalah beralih dari pendinginan berbasis air ke pendinginan berbasis udara di lokasi dengan stres air tinggi. Selain itu, Google juga berkomitmen untuk mencapai 120% replenishment goal, yaitu mengganti lebih banyak air daripada yang mereka konsumsi di beberapa lokasi pusat data.\n",
        "\n",
        ">Pendapat saya:\n",
        "\n",
        ">Menurut saya, mengatasi dampak konsumsi air sebaiknya tidak hanya berfokus pada teknologi pendinginan. Penting juga memperhatikan keberlanjutan sosial di sekitar pusat data. Jika pusat data menggunakan air dalam jumlah besar di daerah yang masyarakatnya kesulitan mendapatkan air bersih, itu bisa menimbulkan konflik. Jadi, saya berpendapat langkah terbaik adalah melakukan assessment sosial dan lingkungan sebelum membangun pusat data, memastikan teknologi pendinginan dipilih sesuai kondisi lokal, serta transparan kepada publik tentang berapa banyak air yang digunakan dan bagaimana dampaknya terhadap masyarakat sekitar.\n",
        "\n",
        ">Kesimpulan Pendapat Pribadi:\n",
        "\n",
        ">Secara pribadi, saya melihat bahwa cara mengatasi dampak lingkungan AI harus dimulai sejak awal desain hingga tahap operasional. Untuk energi, kuncinya adalah desain model dan perangkat keras yang hemat. Untuk emisi, kuncinya ada pada penggunaan energi bersih. Untuk air, kuncinya adalah pemilihan teknologi pendinginan yang sesuai dengan kondisi lokal serta memperhatikan keberlanjutan sosial. Dengan pendekatan ini, AI bisa berkembang tanpa harus meninggalkan beban lingkungan yang berat.\n",
        "\n",
        "\n",
        ">src: https://share.google/P4XwcV8QbOZOKtoGC"
      ],
      "metadata": {
        "id": "uNKI9P6g3Vfd"
      }
    }
  ]
}